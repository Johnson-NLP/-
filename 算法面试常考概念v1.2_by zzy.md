### 总览

![preview](https://pic2.zhimg.com/v2-009013278688f520c070b27910255cb1_r.jpg?source=1940ef5c)

### 常考算法总结

1、排序算法：快速排序、归并排序、计数排序

2、搜索算法：回溯、递归、剪枝技巧

3、图论：最短路、最小生成树、网络流建模

4、动态规划：背包问题、最长子序列、计数问题

5、基础技巧：分治、倍增、二分、贪心

#### 算法复杂度

![image-20200809112158452](/Users/johnson/Library/Application Support/typora-user-images/image-20200809112158452.png)

#### 快速排序性能分析

快速排序的一次划分算法从两头交替搜索，直到low和hight重合，因此其时间复杂度是O(n)；而整个快速排序算法的时间复杂度与划分的趟数有关。

理想的情况是，每次划分所选择的中间数恰好将当前序列几乎等分，经过log2n趟划分，便可得到长度为1的子表。这样，整个算法的时间复杂度为**O(nlog2n)**。 

最坏的情况是，每次所选的中间数是当前序列中的最大或最小元素，这使得每次划分所得的子表中一个为空表，另一子表的长度为原表的长度-1。这样，长度为n的数据表的快速排序需要经过n趟划分，使得整个排序算法的时间复杂度为**O(n2)**。

为改善最坏情况下的时间性能，可采用其他方法选取中间数。通常采用“三者值取中”方法，即比较H->r[low].key、H->r[high].key与H->r[(10w+high)/2].key，取三者中关键字为中值的元素为中间数。

可以证明，快速排序的平均时间复杂度也是**O(nlog2n)**。因此，该排序方法被认为是目前最好的一种内部排序方法。

从空间性能上看，尽管快速排序只需要一个元素的辅助空间，但快速排序需要一个栈空间来实现递归。最好的情况下，即快速排序的每一趟排序都将元素序列均匀地分割成长度相近的两个子表，所需栈的最大深度为log2(n+1)；但最坏的情况下，栈的最大深度为n。这样，快速排序的空间复杂度为**O(log2n))**。



### 数据结构

1、数组与链表：单/双向链表、跳舞链

2、栈与队列

3、树与图：最近公共祖先、并查集

4、哈希表

5、堆：大/小根堆、可并堆

6、字符串：字典树、后缀树



### 计算机基础

#### C/C++基本数据类型所占字节数

1字节=8位

1k=1024字节=2^10

1m=1024k

1g=1024m

char ：1个字节
char*(即指针变量): 8个字节
short int : 2个字节
int： 4个字节
unsigned int : 4个字节
float: 4个字节
double:  8个字节
long:  8个字节
long long: 8个字节
unsigned long: 8个字节

#### C++如何string转char *

主要有三种方法可以将str转换为char*类型，分别是：data(); c_str(); copy()

#### Linux常用指令

查看cpu：top

后台运行：nohup

显示系统进程：ps aux

#### 线程与进程的区别

1、进程是资源分配的最小单位，线程是程序执行的最小单位（资源调度的最小单位）
 2、进程有自己的独立地址空间，每启动一个进程，系统就会为它分配地址空间，建立数据表来维护代码段、堆栈段和数据段，这种操作非常昂贵。
 而线程是共享进程中的数据的，使用相同的地址空间，因此CPU切换一个线程的花费远比进程要小很多，同时创建一个线程的开销也比进程要小很多。
 3、线程之间的通信更方便，同一进程下的线程共享全局变量、静态变量等数据，而进程之间的通信需要以通信的方式（IPC)进行。不过如何处理好同步与互斥是编写多线程程序的难点。
 4、但是多进程程序更健壮，多线程程序只要有一个线程死掉，整个进程也死掉了，而一个进程死掉并不会对另外一个进程造成影响，因为进程有自己独立的地址空间。

#### Python多线程的限制

![image-20200724164714919](/Users/johnson/Library/Application Support/typora-user-images/image-20200724164714919.png)

#### 如何避免受到GIL的影响

##### 用multiprocessing代替Thread

multiprocessing库的出现很大程度上是为了弥补thread库因为GIL而低效的缺陷。它完整的复制了一套thread所提供的接口方便迁移。唯一的不同就是它使用了多进程而不是多线程。每个进程有自己的独立的GIL，因此也不会出现进程之间的GIL争抢。

当然multiprocessing也不是万能良药。它的引入会增加程序实现时线程间数据通讯和同步的困难。就拿计数器来举例子，如果我们要多个线程累加同一个变量，对于thread来说，申明一个global变量，用thread.Lock的context包裹住三行就搞定了。而multiprocessing由于进程之间无法看到对方的数据，只能通过在主线程申明一个Queue，put再get或者用share memory的方法。这个额外的实现成本使得本来就非常痛苦的多线程程序编码，变得更加痛苦了。

##### 用其它解析器

之前也提到了既然GIL只是CPython的产物，那么其他解析器是不是更好呢？没错，像JPython和IronPython这样的解析器由于实现语言的特性，他们不需要GIL的帮助。然而由于用了Java/C#用于解析器实现，他们也失去了利用社区众多C语言模块有用特性的机会。所以这些解析器也因此一直都比较小众。毕竟功能和性能大家在初期都会选择前者，Done is better than perfect。



### 网络

#### TCP

要说http就绕不开tcp，TCP协议对应于传输层，而HTTP协议对应于应用层，从本质上来说，二者没有可比性。但是，http是基于tcp协议的。

#### TCP/IP 协议分层模型

物理层将二进制的0和1和电压高低，光的闪灭和电波的强弱信号进行转换

链路层代表驱动

网络层使用 IP 协议，IP 协议基于 IP 转发分包数据

IP 协议是个不可靠协议，不会重发

IP 协议发送失败会使用ICMP 协议通知失败

ARP 解析 IP 中的 MAC 地址，MAC 地址由网卡出厂提供

IP 还隐含链路层的功能，不管双方底层的链路层是啥，都能通信

传输层

**TCP 协议面向有连接，能正确处理丢包，传输顺序错乱的问题，但是为了建立与断开连接，需要至少7次的发包收包，资源浪费**

**UDP 面向无连接，不管对方有没有收到，如果要得到通知，需要通过应用层**

通用的 TCP 和 UDP 协议

会话层以上分层

TCP/IP 分层中，会话层，表示层，应用层集中在一起

网络管理通过 SNMP 协议

#### TCP三次握手和四次挥手

三次握手：

客户端–发送带有SYN标志的数据包–一次握手–服务端

服务端–发送带有SYN/ACK标志的数据包–二次握手–客户端

客户端–发送带有带有ACK标志的数据包–三次握手–服务端

四次挥手：

客户端-发送一个FIN，用来关闭客户端到服务器的数据传送

服务器-收到这个FIN，它发回一个ACK，确认序号为收到的序号加1 。和SYN一样，一个FIN将占用一个序号

服务器-关闭与客户端的连接，发送一个FIN给客户端

客户端-发回ACK报文确认，并将确认序号设置为收到序号加1

#### HTTP

Http协议是建立在TCP协议基础之上的，当浏览器需要从服务器获取网页数据的时候，会发出一次Http请求。Http会通过TCP建立起一个到服务器的连接通道，当本次请求需要的数据完毕后，Http会立即将TCP连接断开，这个过程是很短的。所以Http连接是一种短连接，是一种无状态的连接。

所谓的无状态，是指浏览器每次向服务器发起请求的时候，不是通过一个连接，而是每次都建立一个新的连接。如果是一个连接的话，服务器进程中就能保持住这个连接并且在内存中记住一些信息状态。而每次请求结束后，连接就关闭，相关的内容就释放了，所以记不住任何状态，成为无状态连接。

![img](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6LzhLS3JISzVpYzZYQjJaTlFiTHgzZVQzNVh1QTFUdUdpYVNKSTdDREJSZEQyU0ZyREx5UUdvTlR1bTZuaGhhcGJWenNQSmlhMTQyTHh3UVQ1QmljMlVUMFhZQS82NDA?x-oss-process=image/format,png)

#### HTTP的英文全称

超文本传输协议（HyperText Transfer Protocol）

#### 状态码

2XX 成功

200 OK，表示从客户端发来的请求在服务器端被正确处理

204 No content，表示请求成功，但响应报文不含实体的主体部分

206 Partial Content，进行范围请求

 

3XX 重定向

301 moved permanently，永久性重定向，表示资源已被分配了新的 URL

302 found，临时性重定向，表示资源临时被分配了新的 URL

303 see other，表示资源存在着另一个 URL，应使用 GET 方法定向获取资源

304 not modified，表示服务器允许访问资源，但因发生请求未满足条件的情况

307 temporary redirect，临时重定向，和302含义相同

 

4XX 客户端错误

400 bad request，请求报文存在语法错误

401 unauthorized，表示发送的请求需要有通过 HTTP 认证的认证信息

403 forbidden，表示对请求资源的访问被服务器拒绝

404 not found，表示在服务器上没有找到请求的资源

 

5XX 服务器错误

500 internal sever error，表示服务器端在执行请求时发生了错误

503 service unavailable，表明服务器暂时处于超负载或正在停机维护，无法处理请求

#### HTTP协议格式

HTTP的请求和响应的消息协议是一样的，分为三个部分，起始行、消息头和消息体。这三个部分以CRLF作为分隔符。最后一个消息头有两个CRLF，用来表示消息头部的结束。

![img](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6LzhLS3JISzVpYzZYQjJaTlFiTHgzZVQzNVh1QTFUdUdpYVNpYm9iajFWT05WUWFBeWlhaWNDZVRjajNsZE9KNndHUTdFMWszV3ZNdzQza21HaWEyYmliQ3B4ZGljbEEvNjQw?x-oss-process=image/format,png)

HTTP请求的起始行称为请求行，形如GET /index.html HTTP/1.1

HTTP响应的起始行称为状态行，形如200 ok

消息头部有很多键值对组成，多个键值对之间使用CRLF作为分隔符，也可以完全没有键值对。形如Content-Encoding: gzip 消息体是一个字符串，字符串的长度是由消息头部的Content-Length键指定的。如果没有Content-Length字段说明没有消息体，譬如GET请求就是没有消息体的，POST请求的消息体一般用来放置表单数据。GET请求的响应返回的页面内容也是放在消息体里面的。我们平时调用API返回的JSON内容都是放在消息体里面的。

#### HTTP的无状态性

所谓HTTP协议的无状态性是指服务器的协议层无需为不同的请求之间建立任何相关关系，它特指的是协议层的无状态性。但是这并不代表建立在HTTP协议之上的应用程序就无法维持状态。

应用层可以通过会话Session来跟踪用户请求之间的相关性，服务器会为每个会话对象绑定一个唯一的会话ID，浏览器可以将会话ID记录在本地缓存LocalStorage或者Cookie，在后续的请求都带上这个会话ID，服务器就可以为每个请求找到相应的会话状态。

#### 输入url到页面加载都发生了什么事情

输入地址

浏览器查找域名的 IP 地址 这一步包括 DNS 具体的查找过程，包括：浏览器缓存->系统缓存->路由器缓存...

浏览器向 web 服务器发送一个 HTTP 请求

服务器的永久重定向响应（从 http://example.com 到 http://www.example.com）

浏览器跟踪重定向地址

服务器处理请求

服务器返回一个 HTTP 响应

浏览器显示 HTML

浏览器发送请求获取嵌入在 HTML 中的资源（如图片、音频、视频、CSS、JS等等）

浏览器发送异步请求



### 算法原理考察

#### 常用的优化器

1. tf.train.Optimizer
2. tf.train.GradientDescentOptimizer （BGD，SGD，MBGD）
3. tf.train.AdadeltaOptimizer
4. tf.train.AdagradOptimizer
5. tf.train.AdagradDAOptimizer
6. tf.train.MomentumOptimizer
7. tf.train.AdamOptimizer （自适应学习率）
8. tf.train.FtrlOptimizer
9. tf.train.ProximalGradientDescentOptimizer 
10. tf.train.ProximalAdagradOptimizer 
11. tf.train.RMSPropOptimizer

#### 模型过拟合的解决方法

L1/L2正则化（原理奥卡姆剃刀）:L2正则化也叫作权重衰减，目标函数中增加所有权重w参数的平方之和，迫使所有w可能趋向0但不为0；L1正则化在损失函数中加入所有权重参数w的绝对值之和，迫使更多的w为0，使特征变得稀疏。
Batch Normalization（对神经网络中下一层的输入进行归一化处理，使得输入量的均值为0，方差为1，即通过特征归一化，加速模型的训练）
shortcut-connect(使用残差网络Residual network)
数据增强(增加样本的数量)
early stopping
Dropout：在训练过程中，让神经元以超参数p的概率被激活(也就是说1-p的概率被设置为0)，类似于bagging算法

#### 如何解决样本类别的不均衡问题

a.过采样/上采样：增加类别少的样本数量实现样本数量的均衡。具体是通过复制类别上的样本构成多条数据。此方法的缺点是当样本的特征很少时，容易出现过拟合。需要对过采样方法进行改进，改进的方法是：在类别少的样本中加入噪声、干扰数据或通过一定的规则产生新合成的样本，如smote算法。
b.欠采样/下采样：减少类别多的样本数量，一般的方法是随机地去掉一些类别多的样本。
c.调整正负样本的惩罚权重：对类别少的样本赋予高的权重，对类别多的样本赋予低的权重。
d.通过集成学习的方法：每次生成训练集时，使用所有类别少的样本，同时从类别多的样本中随机抽取数据与类别少的样本合并起来，构成一个新的训练集。
e.使用特征选择：一般样本不均衡也会导致特征不均衡。但如果类别少的样本量具有一定的规模时，则意味着其特征的分布较为均匀，可以选择出具有显著特征配合参与解决样本不均衡的问题。

#### 在神经网络训练过程中，**为什么会出现梯度消失的问题？如何防止**？

原因：使用了不合适的激活函数，例如sigmoid函数。此时，当神经网络的层数很深时，利用链式求导法则计算梯度时，损失函数的梯度连乘，导致乘积会变得越来越小接近于0，从而神经网络无法学习到新的信息。
解决方法：
预训练加微调
梯度剪切
权重正则化
使用不同的激活函数
使用Batch Normalization
使用残差网络ResNet
使用LSTM网络

#### 介绍一下TensorFlow中的计算图

TensorFlow是一个通过计算图的形式来表述计算的编程系统，计算图也叫作数据流图。可以把计算图看做是一种有向图，TensorFlow中的每个节点都是计算图上的一个张量Tensor，而节点之间的边描述了计算之间的依赖关系和数学运算。

#### K-Means或KNN中，通常使用欧式距离来表示最近的数据点之间的距离，有时候也使用曼哈度距离，对比两者的区别。

![image-20200711164703233](/Users/johnson/Library/Application Support/typora-user-images/image-20200711164703233.png)

#### 参数模型与非参数模型

参数模型：根据预先设计的规则，例如方差损失最小，进行学习，参数模型例子：回归（线性回归、逻辑回归）模型；最好可以看一下或者直接进行一下相关的推导；根据规则，拥有少部分数据就可以；
非参数模型：不需要事先假设规则，直接挖掘潜在数据中的规则；非参数模型例子：KNN，决策树，挖掘数据潜在的特征，所以比较灵活；

#### 生成模型与判别模型

生成模型：根据数据学习联合概率分布P(x,y)，从而计算出条件概率分布P(y|x)作为预测的模型。常用于含有隐变量的模型，例如HMM，朴素贝叶斯算法、高斯混合模型GMM、文档主题生成模型LDA、限制玻尔兹曼机等
判别模型：根据数据直接学习条件概率分布P(x|y)或者决策函数Y=f(X)作为预测模型。例如：逻辑回归、RF、SVM、神经网络、感知机、KNN、CRF等
两者的对比：
使用生成式方法得到的模型，可以还原出模型的联合概率分布，而判别模型不可以；
生成式方法得到的模型收敛速度更快。当样本数增加时，生成式方法得到的模型能更快的收敛到真实模型；
存在隐变量时，只能使用生成模型；
使用判别式方法学习得到的模型，直接面对预测，学习的准确率通常更高，可以简化学习问题。

#### LR和SVM的联系和区别

联系：
LR和SVM都可以处理分类问题，且一般都用于处理线性二分类问题
两个方法都可以增加不同的正则化项，如L1、L2正则化项
区别：
LR是参数模型，SVM是非参数模型
从损失函数来看，LR使用的是交叉熵损失函数，SVM使用的hinge损失函数，这两个损失函数的目的都是增加对分类影响较大的样本点的权重，减小与分类关系比较小的数据点的权重。
SVM的处理方法只考虑支持向量，也就是只考虑和分类最相关的少数样本点来学习分类器。而逻辑回归通过非线性映射，大大减小了离分离超平面远的样本点权重，相对提升了与分类最相关的样本点的权重。
LR模型相对来说简单好理解，一般用于大规模的线性分类。SVM的理解和优化比较复杂，在处理复制非线性分类时，使用核技巧来计算优势明显。
LR能做的SVM也能做，但可能准确率是上有问题，但SVM能做的LR做不了。

#### 神经网络中参数量parameters和FLOPs计算

![image-20200711165120987](/Users/johnson/Library/Application Support/typora-user-images/image-20200711165120987.png)

![image-20200711165211008](/Users/johnson/Library/Application Support/typora-user-images/image-20200711165211008.png)

#### SVM中常见的几种核函数

![image-20200711165252036](/Users/johnson/Library/Application Support/typora-user-images/image-20200711165252036.png)

#### 逻辑回归与线性回归的联系与区别

联系：逻辑回归和线性回归首先都是广义的线性回归；逻辑回归的模型本质上是一个对数线性回归模型，逻辑回归都是以线性回归为理论支持的。但线性回归模型无法做到sigmoid的非线性形式，sigmoid可以轻松处理0/1分类问题。
区别：线性模型的优化目标函数是最小二乘，而逻辑回归则是似然函数
线性回归在整个实数域范围内进行预测，敏感度一致；而分类范围，需要在[0,1]。逻辑回归就是一种减小预测范围，将预测值限定为[0,1]间的一种回归模型，因而对于这类问题来说，逻辑回归的鲁棒性比线性回归的要好。

#### XGBoost为什么要用泰勒公式展开，优势在哪？

XGBoost使用了一阶和二阶偏导, 二阶导数有利于梯度下降的更快更准。使用泰勒展开取得二阶导数形式, 可以在不选定损失函数具体形式的情况下用于算法优化分析.本质上也就把损失函数的选取和模型算法优化和参数选择分开了，这种去耦合增加了XGBoost的适用性。

#### XGBoost如何寻找最优特征？是有放回还是无放回？

XGBoost在训练过程中给各个特征的增益评分，最大增益的特征会被选出来作为分裂的依据，从而记忆了每个特征对在模型训练时的重要性。XGBoost属于boosting的集成学习方法，样本是无放回的，因此每轮计算样本不重复。XGBoost支持子采样，即每轮计算不使用全部样本，以减少过拟合。

#### 决策树、随机森林RF、Boosting、Adaboost、GBDT、XGBoost的区别？

bagging与boosting的区别：
bagging方法有放回的采样相同数量样本训练学习器，然后再一起投票。学习器之间不存在强的依赖关系，学习器可以并行训练生成。集成方式一般为投票法。随机森林属于Bagging的代表，放回抽样，每个学习器随机选择部分特征去优化。
Boosting方法使用全部样本，依次训练每个学习器，迭代集成。学习器之间不存在强依赖关系，学习器可并行训练生成，集成方式为加权和；Adaboost属于Boosting，采用指数损失函数代替原本分类任务中的0-1损失函数；GBDT属于Boosting的优秀代表，对函数残差近似值进行梯度下降，用CRAT树作为基本的学习器，集成模型为回归模型。XGBoost属于Boosting的集大成者，对函数残差近似值进行梯度下降，迭代时利用二阶梯度信息，集成模型可用于分类也可以用于回归。
决策树的学习过程：从根开始建立树，也就是如何选择特征进行分裂。ID3算法使用信息增益、C4.5使用信息增益比、CART树采用基尼系数计算最优分类点，XGBoost使用二阶泰勒展开系数计算最优分裂点。

#### GBDT与XGBoost的对比，XGBoost的优点

损失函数用泰勒展开二项逼近，而不是像GBDT中用的就是一阶导数
对树的结构进行了正则化约束，防止模型过于复杂，降低了过拟合的可能性
节点的分裂方式不同，GBDT使用的是基尼系数，XGBoost使用的是经过优化推导后的算法(穷举法选择最佳的分裂节点、通过加权分位数方法近似选择最佳的分裂节点、针对稀疏特征的分裂点选择法)

#### L1和L2范数的区别

L1 norm:向量中各个元素绝对值之和，也称为稀疏规则算子，L1范数可以使权重稀疏，方便特征提取；L1正则化先验服从拉普拉斯分布
L2 norm:向量中各个元素平方和的1/2次方，又称为Frobenius范数，L2范数可以防止过拟合，提升模型的泛化能力；L2正则化先验服从高斯分布

#### 阐述Adaboost算法的流程

集成学习算法思想:

使用弱分类器和多个样本来构建一个强分类器。AdaBoost是adaptive boosting的缩写，主要运行过程是：首先，对训练数据集中的每个样本进行训练，并赋予每个样本一个权重，这些权重构成一个向量D。一开始，这些样本的初始权重都是相同的！然后，在训练数据上训练出一个弱分类器并计算弱分类器的错误率。接着，在相同的训练数据上再次训练弱分类器。在分类器的第二次训练过程中，将会重新调整每个样本的权重！其中对第一次中分对的样本降低其权重，在第一次中分错的样本提高其权重。为了从所有弱分类器中得到最终的分类结果，AdaBoost还会对每个弱分类器都分配一个权重值alpha,这些alpha值是基于每个弱分类器的错误率进行计算出来的。

![img](https://upload-images.jianshu.io/upload_images/13407176-c6f50166e683fb60.png)

#### LSTM的结构推导，为什么比普通的RNN好？

![image-20200711170844258](/Users/johnson/Library/Application Support/typora-user-images/image-20200711170844258.png)

![image-20200711215400992](/Users/johnson/Library/Application Support/typora-user-images/image-20200711215400992.png)

![image-20200711215456791](/Users/johnson/Library/Application Support/typora-user-images/image-20200711215456791.png)

#### 门控循环单元网络GRU

GRU与LSTM的不同之处在于：**GRU不引入额外的记忆单元![\mathbf{c}_{t}](https://math.jianshu.com/math?formula=%5Cmathbf%7Bc%7D_%7Bt%7D)，GRU网络引入一个更新门来控制当前状态需要从历史状态中保留多少信息(不经过非线性变换)，以及需要从候选状态中接收多少新的信息。**

#### 为什么朴素贝叶斯算法如此朴素？

因为它假设所有的特征在数据集中的作用都是同样重要的，而且相互独立的。这个假设在现实中基本上是不存在的，但特征相关性很小的实际情况还很多，所以这个模型还可以工作的很好。

#### EM算法原理说明

![image-20200711220125026](/Users/johnson/Library/Application Support/typora-user-images/image-20200711220125026.png)

#### GMM算法原理说明

![image-20200711220255976](/Users/johnson/Library/Application Support/typora-user-images/image-20200711220255976.png)

#### KNN算法中K是如何选择的?

如果选择较小的K值，就相当于用较小的邻域中的训练实例进行预测，“学习”近似误差会减小，只有与输入实例较近或相似的训练实例才会对预测结果起作用，与此同时带来的问题是“学习”的估计误差会增大。换句话说，K值的减小就意味着整体模型变得复杂，容易发生过拟合；
如果选择较大的K值，就相当于用较大领域中的训练实例进行预测，其优点是可以减少学习的估计误差，但缺点是学习的近似误差会增大。这时候，与输入实例较远（不相似的）训练实例也会对预测器作用，使预测发生错误，且K值的增大就意味着整体的模型变得简单。
K=N，此时无论输入实例是什么，都只是简单的预测它属于在训练实例中最多的累，模型过于简单，忽略了训练实例中大量有用信息。
实际中，使用交叉验证的方法选择最优的K的取值。
机器学习中，为什么经常需要对数据进行归一化？

#### 机器学习中，为什么经常需要对数据进行归一化？

- 归一化能提高梯度下降算法求解的速度
- 归一化有可能提高精度

#### 神经网络中的批量归一化Batch Normalization(BN)原理

Batch Normalization是批归一化，它可以解决Inner Covariate Shift的问题，那么ICS问题又是什么呢？它是指在网络前向传播的过程中，数据通过激活函数后分布会发生转变，那么下一层网络就要适应这种转变，不仅耗费时间而且梯度方差会变大，对于某些激活函数不友好
使用批归一化之后，每一层的输入都是相同的分布，这也学习时效率变高，而且也在某种程度上避免了梯度爆炸。
分训练和测试来说：

- 训练时，首先计算batch数据的均值和方差，然后进行缩放和平移，即可得到新的样本，新样本既保留有原始的数据分布特征，又有归一化可以加速训练，每一轮靠动量学习均值和方差，留待测试时直接使用
- 测试时，直接使用训练时学到的均值和方差进行归一化

#### 哪些机器学习算法不需要进行归一化操作？

概率模型不需要做归一化操作，因为它们不关心变量的值，而关心的是变量分布和变量之间的条件概率，如决策树。但是，像Adaboost、SVM、LR、KNN、Kmeans等最优化问题就需要归一化。

#### 为什么树形结构不需要归一化？

数值缩放，不影响分裂点位置。因为第一步都是按照特征值进行排序的，排序的顺序不变，那么所属的分支以及分裂点就不会变。对于线性模型，比如说LR，假设有两个特征，一个是(0,1)的，一个是(0,10000)的，这样运用梯度下降时候，损失等高线是一个椭圆的形状，这样想迭代到最优点，就需要很多次迭代，但是如果进行了归一化，那么等高线就是圆形的，那么SGD就会往原点迭代，需要的迭代次数较少。另外，注意树模型是不能进行梯度下降的，因为树模型是阶跃的，阶跃点是不可导的，并且求导没意义，所以树模型（回归树）寻找最优点是通过寻找最优分裂点完成的。

#### 一个完整机器学习项目的流程

- 抽象成数学问题、获取数据、特征预处理与特征选择、训练模型与调优、模型诊断、模型融合、上线运行

#### 条件随机场CRF模型相对于HMM模型(隐马尔科夫模型)和MEMM模型(最大熵隐马尔科夫模型)的优势。

HMM模型中一个最大的缺点即其输出独立性假设，由于输出独立性假设的缺点导致HMM模型不能考虑上下文的特征，限制了特征的选择。
MEMM模型则解决了HMM模型的最大的缺点，可以任意选择特征，但是由于其每一个节点都要进行归一化，所以只能找到局部最优值。同时，也带来了标记偏见的问题即凡是在训练语料库中未出现的情况都被忽略掉了。CRF模型很好的解决了这个问题，它并不在每一节点进行归一化，而是所有特征进行全局归一化，因此可以求出全局的最优值。

#### 什么是熵？

- 熵的定义：离散随机事件的出现概率。一个系统越是有序，信息熵就越低。信息熵可以被认为是系统有序化程度的一个度量。

#### K_Means算法的原理

聚类算法综述：聚类算法是一种无监督学习算法，它是将相似的对象归到同一个簇中。K均值算法中的K可以理解用户想要聚类成K个不同的簇，K是一个用户可以自行定义的超参数。
K均值聚类的优缺点：
优点：容易实现
**缺点：可能收敛到局部最小值，在大规模的数据上收敛慢**
适用场合：数值型数据
**K_Means算法的基本流程**：
1.随机选择K个点作为起始的聚类中心
2.遍历每个样本，计算每个样本到K个聚类中心的距离，找出"距离"聚类中心最近的样本，并将此样本聚集到离它最近的那一个簇中。注：K_Means算法的性能会受到所选距离计算方法的影响。
3.所有样本都聚集到K个簇完成后，计算K个簇的均值，并将聚类中心移动到K个簇的均值处作为新的聚类中心。
4.重复上述步骤2~3，直到最大迭代次数就停止。
K_Means算法的优化(为了克服收敛于局部最小值提出)：如何知道生成的簇比较好？一种用来衡量K_Means算法聚类效果的指标是SSE误差平方和(预测数据与原始数据之间误差的平方和),SSE越小表示样本点越接近于聚类中心点，聚类效果越好。因为对误差取了平方，因此更加重视那些远离聚类中心的点(未理解)。降低SSE值的方法是增加簇的个数，但是簇的个数K在算法一开始运行时就固定了，不能改变。聚类的目标是在保持原有簇数目不变的条件下，提高簇的质量。常用思想是：对生成的簇进行后处理，将具有最大SSE值的簇划分成两个簇。为了保持簇的总数不变，可以将某两个簇进行合并。可以有下面两种方法合并：
1.合并最近的聚类中心：计算所有聚类中心之间的距离，合并距离最近的两个聚类中心点。
2.合并两个使得SSE增加最小的聚类中心：合并两个簇，然后计算总的SSE。必须在所有可能的两个簇上重复上述处理过程，直到找到合并最佳的两个簇。

#### 信息熵增益公式

![image-20200724163624118](/Users/johnson/Library/Application Support/typora-user-images/image-20200724163624118.png)

#### 文本表示方法

分为**离散表示**和**分布式表示**。离散表示的代表就是词袋模型，one-hot（也叫独热编码）、TF-IDF、n-gram都可以看作是词袋模型。**分布式表示也叫做词嵌入（word embedding）**，经典模型是word2vec，还包括后来的Glove、ELMO、GPT和最近很火的BERT。

#### 词袋模型：离散、高维、稀疏

##### One-hot

![image-20200727104628413](/Users/johnson/Library/Application Support/typora-user-images/image-20200727104628413.png)

**第一个问题是数据稀疏和维度灾难**。数据稀疏也就是向量的大部分元素为0，如果词袋中的字词达数百万个，那么由每篇文档转换成的向量的维度是数百万维，由于每篇文档去重后字数较少，因此向量中大部分的元素是0。而且对数百万维的向量进行计算是一件比较蛋疼的事。

但是这样进行文本表示有几个问题。可见，尽管两个句子的长度不一样，但是one-hot编码后长度都一样了，方便进行矩阵运算。

**第二个问题是没有考虑句中字的顺序性**，假定字之间相互独立。这意味着意思不同的句子可能得到一样的向量。比如“我太可爱了，邓紫棋爱我”，“邓紫棋要看我的演唱会”，得到的one-hot编码和上面两句话的是一样的。

**第三个问题是没有考虑字的相对重要性**。这种表示只管字出现没有，而不管出现的频率，但显然一个字出现的次数越多，一般而言越重要（除了一些没有实际意义的停用词）。

##### TF-IDF

TF-IDF用来评估字词对于文档集合中某一篇文档的重要程度。字词的重要性与它在某篇文档中出现的次数成正比，与它在所有文档中出现的次数成反比。TF-IDF的计算公式为：

![image-20200727104742294](/Users/johnson/Library/Application Support/typora-user-images/image-20200727104742294.png)

TF-IDF的思想比较简单，但是却非常实用。然而这种方法还是存在着数据稀疏的问题，也没有考虑字的前后信息。

##### N-gram

上面词袋模型的两种表示方法假设字与字之间是相互独立的，没有考虑它们之间的顺序。于是引入n-gram（n元语法）的概念。n-gram是从一个句子中提取n个连续的字的集合，可以获取到字的前后信息。一般2-gram或者3-gram比较常见。

比如“邓紫棋太可爱了，我爱邓紫棋”，“我要看邓紫棋的演唱会”这两个句子，分解为2-gram词汇表：

｛邓，邓紫，紫，紫棋，棋，棋太，太，太可，可，可爱，爱，爱了，了，了我，我，我爱，爱邓，我要，要，要看，看邓，棋的，的，的演，演，演唱，唱会，会｝

于是原来只有14个字的1-gram字典（就是一个字一个字进行划分的方法）就成了28个元素的2-gram词汇表，词表的维度增加了一倍。

结合one-hot，对两个句子进行编码得到：

[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,0]

[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,1]

也可以结合TF-IDF来得到文本表示，这里不再计算。

这种表示方法的好处是可以获取更丰富的特征，提取字的前后信息，考虑了字之间的顺序性。

但是问题也是显而易见的，这种方法没有解决数据稀疏和词表维度过高的问题，而且随着n的增大，词表维度会变得更高。
 这种表示方法的好处是可以获取更丰富的特征，提取字的前后信息，考虑了字之间的顺序性。

#### 分布式表示：连续、低维、稠密

基本思想是把研究的对象表示成一个低维的稠密的实质的向量，那么这种向量的物理意义就是在于它能够把所有的这些对象都能够表示在一个语义的空间里。

Word2vec是Google的Mikolov等人提出来的一种文本分布式表示的方法，这种方法是对神经网络语言模型的“瘦身”， 巧妙地运用层次softmax（hierarchical softmax ）和负采样（Negative sampling ）两种技巧，使得原本参数繁多、计算量巨大的神经网络语言模型变得容易计算。

Word2vec概括地说是包含了两种模型和两种加速训练方法：

（1）两种模型：CBOW（continuous bag-of-words）和Skip-Gram。**CBOW的目标是通过上下文的词语预测中间的词是什么。而skip-gram则相反，由一个特定的词来预测前后可能出现的词。**这两个模型并非是在Word2vec中首次提出，而是神经网络语言模型中就有的。

（2）两种方法：层次softmax和负采样。层次softmax是通过构建一种有效的树结构（哈夫曼树，huffman tree）来加速计算词语的概率分布的方法；而负采样则是通过随机抽取负样本，与正样本一起参加每次迭代，变成一个二分类问题而减少计算量的方法。

